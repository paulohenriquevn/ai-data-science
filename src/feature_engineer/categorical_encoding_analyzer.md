# Técnicas para Análise e Tratamento de Variáveis Categóricas

| Técnica | Descrição | Vantagens | Desvantagens | Cenários Recomendados | Considerações Estatísticas |
|---------|-----------|-----------|--------------|------------------------|----------------------------|
| **One-Hot Encoding** | Cria uma coluna binária para cada categoria | • Não assume ordenação. Sem perda de informação. Compatível com maioria dos algoritmos | • Aumenta dimensionalidade. "Curse of dimensionality". Problema com alta cardinalidade | • Categorias sem ordem natural. Poucas categorias (< 10). Modelo linear/logístico | • Pode gerar multicolinearidade perfeita (dummy trap). Remover uma categoria de referência |
| **Label Encoding** | Converte categorias em valores numéricos sequenciais | • Preserva dimensionalidade. Eficiente em memória. Simples de implementar | • Impõe ordenação artificial. Distância numérica não tem significado. Pode enganar algoritmos | • Variáveis ordinais (com ordem natural). Algoritmos baseados em árvores. Alta cardinalidade | • Ordem arbitrária pode afetar alguns modelos. Indiferente para modelos baseados em divisões |
| **Binary Encoding** | Converte números das categorias em representação binária | • Reduz dimensionalidade vs one-hot. Eficiência computacional. Atende alta cardinalidade | • Menos interpretável. Codificação menos intuitiva. Perda parcial de informação | • Alta cardinalidade. Restrições de memória. Modelos não sensíveis a ordenação | • Preserva alguma informação de distância. Cada bit pode ser visto como feature |
| **Frequency Encoding** | Substitui categoria pela sua frequência/contagem | • Captura popularidade/raridade. Dimensionalidade preservada. Funciona com alta cardinalidade | • Perda de informação categórica. Categorias podem ter mesma frequência. Sensível a amostragem | • Quando frequência é informativa. Alta cardinalidade. E-commerce, comportamento | • Correlação potencial com target. Considerar log-transform para distribuições enviesadas |
| **Target Encoding** | Substitui categoria pela média da variável alvo | • Muito informativo. Eficiente para alta cardinalidade. Captura relação com target | • Alto risco de data leakage. Overfitting. Requer validação cruzada | • Alta cardinalidade. Competições. Quando há forte relação com target | • Cross-validation obrigatória. Regularização (shrinkage) recomendada. Smoothing para categorias raras |
| **Weight of Evidence (WoE)** | Transforma baseado no log da razão de probabilidade | • Poderoso para classificação binária. Mostra poder preditivo. Lineariza relação com logodds | • Apenas para target binário. Sensível a categorias raras. Requer cálculo cuidadoso | • Modelagem de crédito. Classificação binária. Interpretabilidade necessária | • log(P(pos\|cat)/P(neg\|cat)). Necessita tratamento para zero counts. IV (Information Value) para seleção |
| **Count Encoding** | Substitui categoria pela contagem de ocorrências | • Simples de implementar. Captura raridade. Dimensionalidade preservada | • Similar a frequency encoding. Perda de informação. Sensível ao tamanho do dataset | • Alta cardinalidade. Quando raridade importa. Análise exploratória | • Considerar log para distribuição enviesada. Pode ser combinado com outras técnicas |
| **Feature Hashing (Hashing Trick)** | Aplica função hash para mapear para espaço menor | • Lida com novas categorias. Controle sobre dimensionalidade. Eficiente com memória | • Colisões de hash. Não interpretável. Perda de informação | • Vocabulário aberto/infinito. Processamento de texto. Muito alta cardinalidade | • Compromisso entre dimensionalidade e colisões. Preserva esparsidade. Não há conversão reversa |
| **Embedding** | Aprende representações densas em espaço de baixa dimensão | • Captura relações semânticas. Redução expressiva de dimensionalidade. Eficiente para alta cardinalidade | • Requer muitos dados. Complexidade computacional. Difícil de interpretar | • Categorias com semântica. Deep learning. Texto, produtos, usuários | • Necessita de validação específica. Dimensão do embedding é hiperparâmetro. Pode ser pré-treinado |
| **Leave-One-Out Encoding** | Como target encoding, mas exclui observação atual | • Reduz data leakage. Menos overfitting. Boa para competições | • Computacionalmente intensivo. Complexo de implementar. Estabilidade com dados pequenos | • Alta cardinalidade. Quando target encoding tem leak. Dados suficientes | • k-fold pode ser alternativa. Combinação com regularização. Trade-off bias-variância |
| **Agrupamento (Clustering)** | Agrupa categorias similares | • Reduz cardinalidade. Preserva semântica. Baseado em dados | • Requer medida de similaridade. Subjetividade no número de grupos. Perda de granularidade | • Muito alta cardinalidade. Categorias hierárquicas. Geografia, produtos | • Definição de similaridade é crítica. Validação do agrupamento. k como hiperparâmetro |
| **Tratamento de Categorias Raras** | Agrupa categorias com baixa frequência | • Reduz ruído. Estabiliza estimativas. Previne overfitting | • Perda de especificidade. Definição arbitrária de "raro". Pode esconder padrões importantes | • Distribuição com cauda longa. Alta cardinalidade. Instabilidade em categorias raras | • Definir threshold baseado em dados. Monitorar impacto no modelo. Considerar estatística bayesiana |
| **Dummy Variables + PCA** | One-hot seguido de redução dimensional | • Reduz dimensionalidade. Preserva informação importante. Relações entre categorias | • Complexidade adicional. Menos interpretável. Pipeline mais complexo | • Alta cardinalidade. Correlação entre categorias. Restrições de dimensionalidade | • Componentes podem ser difíceis de interpretar. Variância explicada como critério. Normalização pode ser necessária |
| **CatBoost Encoding** | Encoding baseado em ordenação e médias da target | • Reduz data leakage. Automatizado no CatBoost. Bom para competições | • Específico para CatBoost. Menos interpretável. Necessita ordenação aleatória | • Uso com algoritmo CatBoost. Alta cardinalidade. Quando target encoding falha | • Baseado em médias cumulativas. Ordenação aleatória para treino. Implementação específica |
| **Análise de Correspondência (CA)** | Técnica de projeção similar a PCA para dados categóricos | • Visualização de relações. Captura associações entre categorias. Adaptado para categóricas | • Principalmente exploratório. Complexo de interpretar. Mais usado em estatística | • Tabelas de contingência. Análise exploratória. Pesquisas de mercado | • Interpretação visual importante. Inércia/variância explicada. Análogos a autovalores em PCA |
| **Codificação James-Stein** | Target encoding com regularização bayesiana | • Reduz overfitting. Mais robusto para categorias raras. Estimativa shrinkage | • Matematicamente mais complexo. Necessita parametrização. Menos conhecido | • Categorias com poucos exemplos. Balanço bias-variância. Alternativa robusta ao target encoding | • Baseia-se em estatística bayesiana. Prior global como regularizador. Nível de shrinkage dependente do tamanho da amostra |
| **Entity Embeddings** | Neural networks para aprender representações | • Captura relações não-lineares. Customizável para problema. Integrável com deep learning | • Necessita grande volume de dados. Computacionalmente intensivo. Conhecimento de deep learning | • Deep learning. Dados suficientes. Transfer learning possível | • Dimensionalidade como hiperparâmetro. Pode usar embeddings pré-treinados. Depende da qualidade do treinamento |