# Técnicas de Normalização e Escalonamento de Dados

| Técnica | Fórmula | Características | Vantagens | Desvantagens | Algoritmos Beneficiados | Quando Usar | Implementação |
|---------|---------|----------------|-----------|--------------|------------------------|-------------|---------------|
| **Min-Max Scaling** (Normalização) | $X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$ | • Escala para [0,1]. Preserva a forma da distribuição. Mantém relações entre valores | • Fácil interpretação. Preserva zeros. Lida com dados esparsos. Mantém proporções | • Sensível a outliers. Não lida com novas distribuições futuras. Diferente escala para diferentes features | • KNN. Redes Neurais. Algoritmos baseados em distância. Modelos lineares regularizados | • Quando os limites são conhecidos e significativos. Quando a interpretabilidade é importante. Quando zeros precisam ser preservados. Para visualizações | • `MinMaxScaler` (sklearn). `scale(x, min=0, max=1)` (R). `normalize()` (pandas). Manual: `(x - min)/(max - min)` |
| **Standard Scaling** (Z-score) | $X_{std} = \frac{X - \mu}{\sigma}$ | • Média 0, desvio padrão 1. Não limita amplitude. Centraliza os dados | • Lida bem com outliers moderados. Útil para distribuições aprox. normais. Trata todas as features igualmente | • Não limita valores a uma faixa. Pode ser problemático para distribuições não normais. Não preserva zeros/esparsidade | • Regressão Linear/Logística. SVM. PCA. Clustering. Gradient Descent | • Quando a distribuição é aprox. normal. Para métodos baseados em covariância. Quando os outliers não são extremos. Algoritmos que assumem distribuição normal | • `StandardScaler` (sklearn). `scale()` (R). `(x - mean())/std()` (pandas). Manual: `(x - mean)/std` |
| **MaxAbs Scaling** | $X_{scaled} = \frac{X}{|X|_{max}}$ | • Escala para [-1,1]. Preserva o sinal. Mantém zeros. Não desloca/centraliza | • Preserva esparsidade. Mantém proporções. Bom para dados esparsos. Mantém zeros inalterados | • Sensível a outliers. Não centraliza os dados. Pode não funcionar bem com algoritmos que precisam de centralização | • Classificadores lineares. Sparse models. Algoritmos que preservam zeros. Modelos online | • Para dados esparsos. Quando preservação de zeros é crítica. Quando o sinal é importante. Para grandes conjuntos de dados | • `MaxAbsScaler` (sklearn). Manual: `x / max(abs(x))` |
| **Robust Scaling** | $X_{robust} = \frac{X - Q_2}{Q_3 - Q_1}$ | • Usa medianas e quartis. Robusto a outliers. Não assume distribuição | • Altamente resistente a outliers. Funciona com distribuições não normais. Preserva relações em dados distorcidos | • Pode comprimir demais a parte central. Menos eficiente estatisticamente para dados normais. Menos comum em pipelines padrão | • Qualquer algoritmo com dados com outliers. Regressão robusta. Clustering em dados ruidosos | • Quando há outliers significativos. Dados com distribuições muito assimétricas. Quando a mediana é mais relevante que a média | • `RobustScaler` (sklearn). Manual: `(x - median)/(Q3 - Q1)` |
| **Normalização L1 (Manhattan)** | $X_{L1} = \frac{X}{\sum_{i=1}^{n}{|X_i|}}$ | • Soma dos valores absolutos = 1. Preserva esparsidade. Também chamada de normalização pelo comprimento da cidade | • Robusta a diferenças de escala. Preserva esparsidade. Bom para features dispersas. Útil em processamento de texto | • Interpretação menos intuitiva. Cada feature influenciada por todas as outras. Suscetível a mudanças na dimensionalidade | • Sparse models. NLP (TF-IDF). Modelos lineares com L1. Algoritmos sensíveis a variação de escala | • Para vetores de características muito esparsos. Em NLP e processamento de texto. Quando a esparsidade deve ser preservada | • `normalize(x, norm='l1')` (sklearn). Manual: `x / sum(abs(x))` |
| **Normalização L2 (Euclidiana)** | $X_{L2} = \frac{X}{\sqrt{\sum_{i=1}^{n}{X_i^2}}}$ | • Soma dos quadrados = 1. Normaliza para o comprimento euclidiano. Útil para vetores de características | • Estabiliza o gradiente em ML. Preserva direção do vetor. Útil para similaridade por cosseno. Padrão para embeddings | • Menos intuitiva para interpretação. Não preserva esparsidade tão bem. Cada feature influenciada por todas as outras | • Algoritmos baseados em produtos escalares. SVM. Cosine similarity. K-means | • Para comparações de similaridade. Quando a direção dos vetores importa. Para embeddings e representações vetoriais | • `normalize(x, norm='l2')` (sklearn). Manual: `x / sqrt(sum(x²))` |
| **Log Transform** | $X_{log} = \log(X + c)$ | • Comprime valores altos. Expande valores baixos. Reduz assimetria positiva. Não é escalonamento, mas transformação | • Lida com distribuições muito assimétricas. Atenua o efeito de outliers extremos. Torna relações multiplicativas aditivas | • Requer dados estritamente positivos. Muda interpretação dos coeficientes. Necessita constante c para zeros | • Regressão Linear (após transformação). Árvores de decisão. Modelos lineares generalizados | • Distribuições com assimetria positiva forte. Dados financeiros, populacionais. Variáveis com crescimento exponencial | • `np.log1p(x)` (numpy). `log1p(x)` (R). Manual: `log(x + 1)` |
| **Box-Cox Transform** | $X_{box} = \begin{cases} \frac{X^\lambda - 1}{\lambda}, & \lambda \neq 0 \\ \log(X), & \lambda = 0 \end{cases}$ | • Família de transformações potência. Busca normalizar a distribuição. λ determinado pelos dados. Requer valores positivos | • Corrige assimetria e não-normalidade. Estabiliza variância. Aproxima a normalidade. Otimizada automaticamente | • Apenas para dados positivos. Complexidade computacional. Parâmetro adicional para otimizar. Sensível a outliers | • Modelos paramétricos. Regressão linear. ANOVA. Testes estatísticos | • Quando a normalidade é importante. Para dados positivos com assimetria. Em análises estatísticas formais. Modelos que assumem homocedasticidade | • `boxcox()` (scipy). `preProcess(..., method='BoxCox')` (caret/R). `forecast::BoxCox()` (R) |
| **Yeo-Johnson Transform** | Extensão de Box-Cox para valores negativos | • Similar a Box-Cox. Funciona com números negativos. Parametrizada para diferentes regiões. Busca aproximar normalidade | • Funciona com valores negativos. Flexível para diferentes distribuições. Aproxima normalidade. Forma unificada da transformação | • Complexidade computacional. Menos intuitiva. Interpretação mais difícil. Sensível a outliers | • Similares a Box-Cox. Modelos paramétricos. Regressão linear. Testes estatísticos | • Quando há valores negativos. Distribuições assimétricas. Quando Box-Cox não é aplicável. Para normalização estatística | • `PowerTransformer(method='yeo-johnson')` (sklearn). `preProcess(..., method='YeoJohnson')` (caret/R) |
| **Quantile Transform** | Mapeamento para distribuição uniforme ou normal via quantis | • Transforma para quantis uniformes. Robusto a outliers. Preserva ordem. Pode gerar distribuição normal | • Extremamente robusto a outliers. Funciona com qualquer distribuição. Uniformiza a influência dos valores. Opção de saída normal | • Perda de informação absoluta. Distorce relações entre variáveis. Menor interpretabilidade. Pode exagerar diferenças pequenas | • Algoritmos sensíveis a distribuição. Random Forests. Gradient Boosting. Redes Neurais | • Quando a forma da distribuição é mais importante que os valores. Para dados muito assimétricos ou com outliers. Quando a uniformidade é desejada | • `QuantileTransformer` (sklearn). `qnorm(ecdf(x)(x))` (R) |
| **Unit Vector Transform** | Divisão por norma, similar a L2 | • Escalamento para comprimento unitário. Preserva a direção. Normalização de vetores | • Foco na direção, não magnitude. Útil para comparação de similaridade. Menos sensível a outliers nas magnitudes | • Perda da informação de magnitude. Não preserva relacionamentos proporcionais. Menos intuitiva para interpretação | • Cosine similarity. Text mining. Clustering baseado em direção. K-means | • Quando apenas a direção do vetor importa. Para comparações de similaridade. Análise de texto e documentos. Em embeddings | • `normalize()` (sklearn). Manual: `x / ||x||` |
| **Mean Normalization** | $X_{mean} = \frac{X - \mu}{X_{max} - X_{min}}$ | • Centraliza em torno de zero. Escala usando amplitude. Combinação de centralização e normalização | • Centraliza os dados. Mantém a escala proporcional à amplitude. Valores em aproximadamente [-1, 1] | • Sensível a outliers. Menos comum que min-max ou z-score. Intervalo não garantido exato | • Algoritmos de gradiente descendente. Redes neurais. Algoritmos de distância | • Quando tanto centralização quanto escala são importantes. Alternativa ao z-score quando a amplitude é mais relevante | • Manual: `(x - mean)/(max - min)` |
| **Sigmoidal / Logistic Transform** | $X_{sig} = \frac{1}{1 + e^{-X}}$ | • Comprime para [0,1]. Suavização de extremos. Útil como ativação. Não linear | • Comprime outliers. Útil para probabilidades. Saída limitada e suave. Transição gradual | • Saturação nos extremos. Perda de resolução nas caudas. Não preserva relações lineares. Pode causar vanishing gradient | • Redes neurais. Modelos de probabilidade. Classificação binária. Feature engineering | • Para transformação de features específicas. Como passo de ativação. Quando valores limitados são desejados. Para conversão em probabilidades | • `expit()` (scipy). Manual: `1/(1+exp(-x))` |
| **Decimal Scaling** | $X_{dec} = \frac{X}{10^j}$ | • Divide por potência de 10. Mantém sinais e zeros. Simples e interpretável | • Mantém proporções exatas. Fácil de entender e implementar. Preserva zeros e sinais. Interpretabilidade | • Não padroniza completamente. Não otimizado estatisticamente. Pode deixar escalas muito diferentes. Arbitrariedade na escolha de j | • Exploração de dados. Aplicações de negócios. Quando a interpretabilidade é crítica | • Para fins de visualização. Análise preliminar. Quando a fácil interpretação é crucial. Para transformação rápida | • Manual: `x / (10^j)` onde j é suficiente para max(abs(x)) < 1 |
| **Escalonamento Adaptativo** | Varia por característica/grupo | • Escolhe técnica por feature. Adapta a transformações. Otimiza para distribuição. Pipeline heterogêneo | • Otimizado para cada feature. Melhor desempenho geral. Adaptado às características dos dados. Flexível | • Complexidade de implementação. Necessidade de validação. Possível overfitting. Dificuldade de manutenção | • Ensemble models. Sistemas híbridos. Competições de ML. Problemas complexos | • Datasets heterogêneos. Quando performance é crítica. Com recursos computacionais disponíveis. Para competições | • Pipelines personalizados. `ColumnTransformer` (sklearn). Feature unions. Transformadores customizados |
| **Power Transform** | $X_{pow} = X^p$ | • Transforma usando potência. Reduz assimetria. Especial: raiz quadrada (p=0.5) | • Estabiliza variância. Simples para certas distribuições. Casos especiais bem conhecidos. Interpretável para certos p | • Restrições para valores negativos. Escolha de p não óbvia. Pode exagerar pequenas diferenças. Comportamento variável | • Modelos lineares. Testes estatísticos. Regressão. Análises de variância | • Dados com assimetria moderada. Quando relações de potência são esperadas. Para estabilizar variância. Distribuições específicas | • `PowerTransformer` (sklearn). Manual: `x^p` (p comum: 0.5, 0.33, 0.25) |