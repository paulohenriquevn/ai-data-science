# Técnicas de Balanceamento de Classes para Dados Desbalanceados

| Técnica | Categoria | Descrição | Vantagens | Desvantagens | Cenários Recomendados | Métricas de Avaliação |
|---------|-----------|-----------|-----------|--------------|----------------------|----------------------|
| **Random Undersampling** | Undersampling | Remove aleatoriamente exemplos da classe majoritária | • Simples e rápido. Reduz custo computacional. Diminui problemas de armazenamento | • Perda de informações potencialmente úteis. Pode remover exemplos importantes. Pode aumentar variância | • Datasets muito grandes. Quando há muitos exemplos da classe majoritária. Para testes rápidos iniciais | • F1-score. ROC-AUC. Precision-Recall AUC |
| **NearMiss** | Undersampling | Seleciona exemplos da classe majoritária baseado na distância aos exemplos da minoritária | • Preserva exemplos da fronteira. Mantém informação discriminativa. Mais informativo que random | • Sensível a ruído. Computacionalmente intensivo em grandes datasets. Pode criar distribuições artificiais | • Quando a fronteira de decisão é importante. Para problemas onde a semântica espacial é relevante | • F1-score. Recall. G-mean |
| **Tomek Links** | Undersampling | Remove pares de exemplos de classes diferentes que são os vizinhos mais próximos entre si | • Remove ruído e sobreposição. Clarifica fronteiras de decisão. Preserva estrutura dos dados | • Pode remover poucos exemplos. Computa distâncias par a par (O(n²)). Pode não ser suficiente sozinho | • Para limpeza de dados. Como pré-processamento. Combinado com outras técnicas | • F1-score. Precision. ROC-AUC |
| **Condensed Nearest Neighbors (CNN)** | Undersampling | Seleciona subset mínimo que classifica corretamente com 1-NN | • Preserva características discriminativas. Reduz significativamente o dataset. Mantém fronteira de decisão | • Sensível à ordem dos dados. Computacionalmente intensivo. Não lida bem com ruído | • Redução substancial de dados. Quando a fronteira de decisão é bem definida | • Accuracy. F1-score. Taxa de redução |
| **Random Oversampling** | Oversampling | Duplica aleatoriamente exemplos da classe minoritária | • Simples e intuitivo. Não perde informação. Implementação fácil | • Risco de overfitting. Duplicação exata (não cria variabilidade). Pode aumentar tempo de treino | • Conjuntos de dados pequenos. Quando undersampling causa perda significativa. Testes preliminares | • F1-score. Recall. Precision-Recall AUC |
| **SMOTE** (Synthetic Minority Oversampling Technique) | Oversampling | Gera exemplos sintéticos interpolando entre exemplos minoritários vizinhos | • Cria exemplos novos, não duplicados. Generaliza o espaço de características. Evita overfitting em comparação com duplicação | • Pode gerar ruído em áreas sobrepostas. Ignora a classe majoritária na geração. Difícil em dados categóricos | • Quando há poucos exemplos da classe minoritária. Para expandir o espaço de características. Dados numéricos | • F1-score. ROC-AUC. Recall |
| **Borderline-SMOTE** | Oversampling | Aplica SMOTE apenas a exemplos minoritários próximos à fronteira de decisão | • Foca na região de decisão mais difícil. Mais eficiente que SMOTE regular. Melhor generalização | • Mais complexo que SMOTE regular. Depende da definição de "fronteira". Pode amplificar ruído na fronteira | • Quando a fronteira de decisão é crítica. Para melhorar a discriminação entre classes | • F1-score. G-mean. ROC-AUC |
| **ADASYN** | Oversampling | Gera mais exemplos sintéticos em regiões difíceis de aprender | • Adaptativo ao nível de dificuldade. Foco em exemplos difíceis. Reduz viés de classificação | • Computacionalmente mais intensivo. Sensível a parâmetros. Pode criar ruído | • Problemas complexos com distribuição não uniforme. Quando há muitas regiões difíceis | • F1-score. Balanced accuracy. G-mean |
| **SMOTE + Tomek Links** | Híbrido | Aplica SMOTE e depois remove Tomek links | • Cria novos exemplos e limpa sobreposições. Melhora a definição da fronteira. Combina vantagens das duas abordagens | • Complexidade de implementação. Custo computacional maior. Múltiplos parâmetros | • Datasets com sobreposição. Quando tanto under quanto oversampling isolados são insuficientes | • F1-score. ROC-AUC. Precision-Recall AUC |
| **SMOTETomek** | Híbrido | Implementação integrada de SMOTE + Tomek Links | • Otimizado para execução conjunta. Melhor balanço entre classes. Interface única | • Similar ao SMOTE + Tomek separados. Múltiplos hiperparâmetros. Complexidade computacional | • Disponível em bibliotecas como imbalanced-learn. Para workflow padronizado | • F1-score. ROC-AUC. Balanced accuracy |
| **SMOTE + ENN** | Híbrido | Aplica SMOTE e depois limpa com Edited Nearest Neighbors | • Remoção mais agressiva de ruído. Melhor definição de fronteira. Remove exemplos majoritários e minoritários problemáticos | • Pode reduzir muito os dados. Computacionalmente intensivo. Sensível ao parâmetro k | • Datasets com muito ruído. Quando a limpeza é prioritária. Para fronteiras bem definidas | • F1-score. G-mean. Recall |
| **Class Weights** | Algoritmo | Atribui pesos maiores à classe minoritária durante o treinamento | • Preserva todos os dados. Fácil de implementar. Não modifica o dataset | • Não resolve problemas de representatividade. Nem todos algoritmos suportam. Difícil calibração de pesos | • Algoritmos que suportam pesos (SVM, RF, etc.). Quando a modificação dos dados é indesejada | • F1-score ponderado. Balanced accuracy. ROC-AUC |
| **Cost-Sensitive Learning** | Algoritmo | Incorpora diferentes custos de erro no algoritmo | • Otimiza diretamente para o objetivo. Não modifica distribuição. Captura importância do negócio | • Requer conhecimento dos custos. Implementação específica por algoritmo. Pode ser difícil de calibrar | • Quando os custos de erro são conhecidos. Cenários como fraude ou diagnóstico. Para otimização direta do objetivo de negócio | • Custo total. F1-score. Profit/Loss específico |
| **Ensemble com Balanceamento** | Híbrido | Treina múltiplos modelos em subconjuntos balanceados | • Utiliza todos os dados. Reduz variância. Cada modelo vê distribuição diferente | • Computacionalmente intensivo. Complexo de implementar. Difícil de interpretar | • Conjuntos de dados grandes. Quando há recursos computacionais. Para maximizar performance | • F1-score. ROC-AUC. Specificity/Sensitivity |
| **Bagging-based (BalancedBaggingClassifier)** | Ensemble | Aplica undersampling em cada bootstrap do bagging | • Reduz variância. Utiliza diferentes subconjuntos. Implementação disponível | • Necessita múltiplos modelos. Maior tempo de treino e inferência. Pode ser sensível a outliers | • Quando há variância alta com undersampling. Datasets médios a grandes | • F1-score. ROC-AUC. Balanced accuracy |
| **EasyEnsemble** | Ensemble | Combina AdaBoost com undersampling | • Foca em exemplos difíceis. Reduz viés e variância. Boa generalização | • Complexidade computacional. Múltiplos hiperparâmetros. Sensível a ruído | • Quando há recursos computacionais. Para alta performance. Em problemas complexos | • F1-score. ROC-AUC. G-mean |
| **RUSBoost** | Ensemble | Combina boosting com random undersampling | • Eficiente computacionalmente. Foco adaptativo em exemplos difíceis. Bom para grandes datasets | • Pode sofrer com perda de informação. Sensível à ordem de remoção. Pode aumentar variância | • Grandes conjuntos de dados. Quando o random undersampling funciona bem | • F1-score. ROC-AUC. G-mean |
| **Threshold Moving** | Pós-processamento | Ajusta o limiar de decisão após treinamento | • Não modifica o treinamento. Flexível, pode ser ajustado. Preserva distribuição de probabilidade | • Requer probabilidades calibradas. Não melhora a representatividade. Difícil calibração automática | • Como ajuste final. Para modelos que produzem probabilidades. Validação com stakeholders | • F1-score. ROC-AUC. Profit/Loss específico |
| **One-Class Learning** | Algoritmo | Treina apenas com exemplos da classe majoritária ou minoritária | • Útil para detecção de anomalias. Robusto quando uma classe é muito rara. Não depende de balanceamento | • Perda de informação discriminativa. Requer algoritmos específicos. Difícil calibração | • Detecção de anomalias/fraudes. Classes extremamente desbalanceadas (< 1%). Quando uma classe é bem definida | • AUC. Precision@K. False positive rate |