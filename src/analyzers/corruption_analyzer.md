# Técnicas para Detecção e Tratamento de Dados Corrompidos ou Inconsistentes

| Tipo de Problema | Técnicas de Detecção | Técnicas de Tratamento | Implicações Analíticas | Ferramentas/Bibliotecas | Melhores Práticas |
|------------------|----------------------|------------------------|--------------------------|--------------------------|-------------------|
| **Valores Impossíveis** | • Validação por regras de domínio. Análise de limites físicos. Histogramas e boxplots. Verificação de restrições lógicas | • Substituição por valores plausíveis. Imputação estatística. Flagging e exclusão. Correção baseada em regras de negócio | • Distorção em estatísticas. Resultados implausíveis. Erros em previsões. Perda de credibilidade da análise | • Great Expectations. Pandas Profiling. Cerberus. Pydantic | • Documentar regras de validação. Estabelecer protocolos de correção. Rastreabilidade das alterações. Consultar especialistas do domínio |
| **Inconsistências entre Variáveis** | • Validação de relações cruzadas. Verificação de restrições de integridade. Análise de correlação condicional. Testes de consistência lógica | • Revisão conjunta de variáveis. Reconciliação baseada em regras. Priorização por confiabilidade. Exclusão em caso de ambiguidade sem solução | • Resultados contraditórios. Modelos com baixa performance. Falhas em validação cruzada. Distorções em análise multivariada | • Pandas. data-validator. Deequ. Apache Griffin | • Mapear dependências entre variáveis. Criar matrizes de validação cruzada. Documentar hierarquia de confiabilidade. Auditoria regular de consistência |
| **Erros de Digitação** | • Análise de frequência. Distância de edição. Verificação ortográfica. Clustering de strings similares | • Correção automatizada para casos simples. Fuzzy matching. Padronização manual para casos críticos. Dicionários de correção | • Fragmentação de categorias. Perda de poder estatístico. Falsa diversidade. Problemas em joins/merges | • FuzzyWuzzy. OpenRefine. recordlinkage. textdistance | • Criar dicionários de termos aceitos. Implementar validação na entrada. Usar autocorreção sugestiva. Monitorar frequência de categorias raras |
| **Inconsistências de Formato** | • Validação por regex. Verificação de padrões. Análise de tipo de dados. Detecção de anomalias estruturais | • Parser especializado. Normalização de formato. Extração estruturada. Parsing condicional | • Erros de tipo<br>• Problemas de conversão<br>• Falhas em cálculos<br>• Inconsistências em agrupamentos | • regex<br>• arrow<br>• dateutil<br>• phonenumbers | • Padronizar formatos em guias de estilo<br>• Implementar parseamento defensivo<br>• Validar antes de converter<br>• Documentar formatos aceitos |
| **Duplicatas** | • Identificação exata<br>• Matching probabilístico<br>• Análise de chaves compostas<br>• Deduplicação fuzzy | • Remoção de duplicatas exatas<br>• Consolidação de registros<br>• Criação de identificadores únicos<br>• Merge com resolução de conflitos | • Viés de amostragem<br>• Sobrerrepresentação<br>• Inflação de métricas<br>• Problemas em análise temporal | • pandas.duplicated<br>• dedupe<br>• recordlinkage<br>• Elasticsearch (fuzzy) | • Definir claramente regras de unicidade<br>• Verificar em múltiplos níveis<br>• Documentar decisões de merge<br>• Criar índices de unicidade |
| **Inconsistências Temporais** | • Verificação de sequência lógica<br>• Análise de intervalos impossíveis<br>• Validação de fusos horários<br>• Detecção de anacronismos | • Correção de ordem temporal<br>• Normalização de timezone<br>• Padronização de formato<br>• Imputação baseada em sequência | • Erros em análise de séries temporais<br>• Causalidade reversa<br>• Inferência incorreta<br>• Anomalias em tendências | • pandas Timestamp<br>• arrow<br>• dateutil<br>• pytz | • Padronizar para UTC internamente<br>• Documentar granularidade temporal<br>• Validar transições de estado<br>• Implementar verificações de sequência |
| **Valores Atípicos (Outliers)** | • Z-score<br>• IQR (intervalo interquartil)<br>• DBSCAN<br>• Isolation Forest | • Winsorização<br>• Transformação robusta<br>• Exclusão justificada<br>• Tratamento separado | • Distorção em médias<br>• Inflação de variância<br>• Instabilidade em modelos<br>• Viés em coeficientes | • scikit-learn<br>• PyOD<br>• statsmodels<br>• scipy.stats | • Diferenciar entre erros e valores reais raros<br>• Análise contextual<br>• Documentar critérios de identificação<br>• Validar com especialistas |
| **Codificação Incorreta** | • Verificação de encoding. Análise de caracteres estranhos. Padrões inconsistentes. Frequência anormal de caracteres | • Recodificação explícita. Normalização Unicode. Substituição de caracteres inválidos. Limpeza de BOM e outros marcadores | • Problemas em processamento de texto. Erros em NLP. Falsos positivos em busca. Inconsistências em extração | • chardet. ftfy. unicodedata. BeautifulSoup | • Padronizar UTF-8 em todo pipeline. Verificar BOM em arquivos. Implementar detecção automática. Validar após conversões |
| **Valores Truncados ou Incompletos** | • Análise de distribuição. Verificação de limites. Padrões de truncamento. Análise de comprimento | • Reconstrução por modelagem. Imputação baseada em padrões. Flags de incompletude. Limites de confiança | • Subestimação de variabilidade. Viés estatístico. Falsos padrões. Suavização artificial | • pandas. numpy. impyute. scikit-learn | • Documentar padrões de truncamento. Analisar impacto na distribuição. Considerar censura estatística. Validar reconstruções |
| **Inconsistências entre Fontes** | • Reconciliação de fontes cruzadas. Análise de discrepâncias. Rastreamento de proveniência. Validação cruzada | • Hierarquia de confiabilidade. Consolidação ponderada. Metadados de fonte. Resolução de conflitos documentada | • Resultados contraditórios. Problemas de reproducibilidade. Fragmentação analítica. Dificuldade em auditoria | • data-integration tools. Apache Nifi. Talend. Apache Airflow | • Documentar linhagem de dados. Estabelecer fonte autoritativa. Implementar reconciliação regular. Criar métricas de consistência |
| **Problemas de Escala/Unidade** | • Análise de distribuição por grupo. Verificação de magnitude. Comparação com valores de referência. Análise de proporcionalidade | • Conversão padronizada. Normalização de unidades. Metadados de escala. Transformação com verificação | • Comparações inválidas. Erros de magnitude. Correlações falsas. Resultados implausíveis | • Pint. pandas. quantulum. numerizer | • Padronizar unidades em todo pipeline. Implementar verificações de plausibilidade. Documentar fatores de conversão. Validar após transformações |
| **Censura de Dados** | • Análise de truncamento. Padrões em valores máximos/mínimos. Detecção de heaping. Análise de densidade em limites | • Métodos estatísticos para dados censurados. Modelos Tobit. Maximum likelihood estimation. Imputação adaptada | • Subestimação de variância. Viés em parâmetros. Redução de poder estatístico. Distorção em extremos | • lifelines. survival. censored_regressor. statsmodels | • Documentar mecanismos de censura. Implementar métodos específicos. Analisar sensibilidade. Reportar limitações |
| **Valores Mascarados (Privacy)** | • Auditoria de privacidade. Verificação de consistência. Análise de entropia. Detecção de padrões artificiais | • Desanonimização controlada. Técnicas de privacidade diferencial. Análise em ambiente seguro. Modelagem com limitação | • Relações artificiais. Perda de granularidade. Redução de variância. Limitações em inferência | • ARX Data Anonymization. Diffprivlib. Cryptopy. OpenPseudonymiser | • Balancear utilidade e privacidade. Documentar métodos de anonimização. Estabelecer níveis de acesso. Validar impacto analítico |
| **Dados Corrompidos em Armazenamento** | • Checksums. Verificação de integridade. Detecção de bits corrompidos. Validação estrutural | • Recuperação de backups. Reconstrução parcial. Inferência de valores. Exclusão documentada | • Perda de informação. Viés por exclusão. Problemas de reproducibilidade. Amostras não representativas | • hashlib. fsck tools. file recovery tools. data validation frameworks | • Implementar verificação regular. Manter backups incrementais. Documentar processos de validação. Estratégias de degradação graciosa |
| **Inconsistências Semânticas** | • Validação ontológica. Verificação de taxonomia. Análise de relações conceituais. Verificação de consistência lógica | • Harmonização semântica. Mapeamento ontológico. Correção baseada em regras. Reconciliação de conceitos | • Erros de interpretação. Agregações incorretas. Problemas em ML explicável. Inconsistências em reports | • OWL tools. Protégé. SKOS frameworks. semantic-py | • Desenvolver ontologias de domínio. Documentar mapeamentos conceituais. Validar com especialistas. Implementar verificação contínua |
| **Registros Fantasmas/Zumbis** | • Auditoria de entidade. Verificação de estados terminais. Análise de ciclo de vida. Reconciliação com sistemas externos | • Desativação baseada em regras. Marcação explícita de status. Consolidação de identidade. Purga controlada | • Viés de sobrevivência. Distorção em métricas. Falsos contatos/clientes. Problemas em reconciliação | • Entity resolution tools. Master data management. Data quality frameworks. Lineage tracking | • Implementar estados explícitos. Criar processos de arquivamento. Auditar ciclos de vida regularmente. Documentar critérios de atividade |
| **Erros de Cálculo Persistentes** | • Reconciliação de derivações. Auditoria de fórmulas. Verificação de precisão numérica. Teste com valores conhecidos | • Recálculo com algoritmos corretos. Atualização em cascata. Correção de dependências. Documentação de mudanças | • Decisões baseadas em dados incorretos. Propagação de erros. Inconsistências em relatórios. Problemas em análises comparativas | • numpy. mpmath. pandas. data lineage tools | • Documentar todas as fórmulas. Implementar testes unitários. Validar com casos conhecidos. Auditoria periódica |