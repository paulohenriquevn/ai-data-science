# Técnicas para Detecção e Tratamento de Dados Duplicados

| Tipo de Duplicação | Técnicas de Detecção | Estratégias de Tratamento | Desafios | Ferramentas/Bibliotecas | Métricas de Avaliação | Melhores Práticas |
|--------------------|----------------------|---------------------------|----------|-------------------------|------------------------|-------------------|
| **Duplicatas Exatas** | • Comparação direta de registros. Hashing de registros. Ordenação e comparação sequencial. Contagem de valores distintos | • Remoção de duplicatas (`drop_duplicates`). Manutenção do primeiro/último registro. Agregação por ID. Criação de flags de duplicação | • Volume de dados. Performance em grandes datasets. Identificação de chaves primárias. Coordenação entre sistemas | • pandas (drop_duplicates). SQL (DISTINCT, GROUP BY). Apache Spark (dropDuplicates). dataset.dedupe() (Python) | • Contagem de duplicatas. Taxa de redução. Tempo de processamento. Overhead de armazenamento | • Criar índices para chaves. Documentar regras de remoção. Manter logs de deduplicação. Implementar validação na entrada |
| **Duplicatas Parciais** | • Agrupamento por chaves parciais. Técnicas de blocking. Comparação de subconjuntos de colunas. Análise de valores NULL | • Priorização de registros completos. Mesclagem de informações complementares. Resolução baseada em regras. Preenchimento condicional | • Definição de regras de mesclagem. Identificação de registros prioritários. Informações conflitantes. Rastreabilidade | • pandas (groupby + agg). recordlinkage. dataprep. Apache Griffin | • Cobertura de dados. Taxa de informação preservada. Conflitos resolvidos. Qualidade dos registros mesclados | • Estabelecer hierarquia de confiabilidade. Documentar lógica de mesclagem. Validar após mesclagem. Manter versão original |
| **Duplicatas Fuzzy (Aproximadas)** | • Similaridade de strings (Levenshtein, Jaccard). Phonetic matching (Soundex, Metaphone). N-gram comparison. TF-IDF com cosine similarity | • Clustering por similaridade. Correção manual assistida. Consolidação com limiar. Deduplicação iterativa | • Definição de limiares. Falsos positivos/negativos. Escalabilidade. Complexidade computacional | • FuzzyWuzzy. recordlinkage. dedupe. stringdist | • Precision/Recall. F1-score. Taxa de falsos positivos. Taxa de falsos negativos | • Testar limiares em amostra. Implementar revisão manual. Usar múltiplas técnicas. Balancear precisão e cobertura |
| **Duplicatas Multi-Fonte** | • Linkage de registros. Canonicalização de fontes. Análise de proveniência. Detecção de entidade comum | • Sistema de entidade mestra (MDM). Hierarquia de fontes. Timestamps de atualização. Consolidação por regras de negócio | • Diferentes formatos. Esquemas distintos. Conflitos de valores. Temporalidade | • Talend MDM. d6tstack. link-rex. splink | • Cobertura entre fontes. Consistência de valores. Taxa de resolução. Pureza dos clusters | • Estabelecer fonte autoritativa. Padronizar formatos. Mapear campos entre sistemas. Documentar fluxo de dados |
| **Duplicatas com Valores Diferentes** | • Análise de chaves naturais. Detecção de conflitos. Verificação de consistência temporal. Análise de dependências | • Resolução baseada em regras. Escolha do registro mais recente. Consolidação por completude. Criação de versões | • Identificação de valor correto. Registros parcialmente atualizados. Propagação de correções. Ciclo de vida dos dados | • pandas (groupby + apply). data-wrangling tools. ETL platforms. Custom conflict resolution | • Taxa de conflitos. Consistência após resolução. Divergência de valores. Qualidade das decisões | • Documentar regras de prioridade. Manter histórico de valores. Implementar versionamento. Validar integridade pós-resolução |
| **Duplicatas Hierárquicas** | • Análise de relacionamentos. Travessia de hierarquia. Detecção de ciclos. Propagação de IDs | • Normalização hierárquica. Consolidação em níveis. Resolução top-down ou bottom-up. Restruturação de árvores | • Relações complexas. Propagação de mudanças. Integridade referencial. Ciclos e recursão | • NetworkX. tree-based algorithms. Neo4j. hierarch (Python) | • Consistência hierárquica. Preservação de relações. Profundidade da resolução. Integridade estrutural | • Mapear toda a hierarquia. Resolver de cima para baixo. Manter referências. Validar estrutura resultante |
| **Duplicatas Temporais** | • Análise de séries temporais. Detecção de registros periódicos. Timestamps similares. Padrões de replicação | • Agregação temporal. Remoção por janela. Amostragem controlada. Consolidação por intervalo | • Granularidade temporal. Fusos horários. Frequência de amostragem. Continuidade dos dados | • pandas (resample). tslearn. cesium. darts | • Preservação de tendências. Detecção de sazonalidade. Redução de ruído. Fidelidade da série | • Padronizar timestamps. Definir política de amostragem. Considerar agregações. Validar antes/depois |
| **Duplicatas de Eventos** | • Detecção de eventos similares. Análise de sequência. Clustering temporal. Validação de unicidade | • Deduplicação por janela. Consolidação de eventos. Filtragem por limiar. Flag de repetição | • Definição de evento único. Eventos relacionados vs duplicados. Causalidade vs correlação. Sequência vs simultaneidade | • Process mining tools. Event processing systems. Apache Flink. Esper | • Pureza de eventos. Preservação de sequência. Taxa de compressão. Qualidade da cadeia causal | • Definir claramente eventos. Estabelecer janelas de deduplicação. Preservar causalidade. Documentar regras de consolidação |
| **Duplicatas em Dados Estruturados** | • Schema validation. Constraints de unicidade. Análise de chaves primárias/estrangeiras. Verificação de índices | • Reforço de constraints. Normalização de esquema. Correção de chaves. Implementação de integridade referencial | • Migração de dados legados. Schemas flexíveis. Sistemas distribuídos. Performance vs integridade | • SQL constraints. Schema validators. Database tools. ORM frameworks | • Violações de unicidade. Integridade referencial. Normalização. Performance de queries | • Implementar constraints. Criar índices únicos. Validar na entrada. Auditar periodicamente |
| **Duplicatas em Dados Semi-estruturados** | • Path analysis em XML/JSON. Detecção de objetos similares. Hash de subestruturas. Normalização de esquemas | • Reestruturação de dados. Normalização de formato. Deduplicação de sub-elementos. Consolidação hierárquica | • Estruturas aninhadas. Flexibilidade de schema. Variações de representação. Evolução de estrutura | • jsonschema. xmldiff. jsondiff. MongoDB tools | • Compactação estrutural. Consistência de schema. Preservação de relações. Integridade de dados | • Validar contra schema. Normalizar representações. Implementar versões de schema. Documentar transformações |
| **Duplicatas Transacionais** | • Validação de sequência. Detecção de reprocessamento. Análise de IDs de transação. Verificação de idempotência | • Processamento idempotente. Registro de transações processadas. Confirmação de transação. Rollback de duplicatas | • Falhas de rede. Retry logic. Processamento distribuído. Consistência eventual | • Distributed transaction systems. Message queues. Database transactions. Idempotency libraries | • Taxa de duplicação. Consistência do estado final. Tempo de detecção. Resiliência a falhas | • Implementar idempotência. Usar IDs de transação. Manter log de transações. Implementar confirmação |
| **Duplicatas em Migração de Dados** | • Verificação de completude. Validação de mapeamento. Controle de versão de migração. Checksums e contagens | • Estratégia de migração faseada. Verificação pré/pós migração. Reconciliação de dados. Limpeza pós-migração | • Diferenças de schema. Volumes de dados. Janelas de manutenção. Verificação de integridade | • ETL tools. Database migration utilities. Validation frameworks. Data reconciliation tools | • Taxa de sucesso. Integridade pós-migração. Tempo de migração. Cobertura de dados | • Planejar verificações. Executar em fases. Manter ambiente paralelo. Validar após cada fase |
| **Duplicatas de Usuários/Entidades** | • Matching de atributos. Análise de comportamento. Detecção de padrões de uso. Verificação de contato | • Sistema MDM. Merge de perfis. Notificação ao usuário. Consolidação progressiva | • Privacidade e GDPR. Consentimento de união. Fragmentação de informação. Experiência do usuário | • Entity resolution systems. CRM deduplication tools. Identity management. Customer data platforms | • Unificação de perfis. Satisfação do usuário. Qualidade dos dados consolidados. Completude da visão | • Priorizar consentimento. Notificar usuários. Implementar fusão gradual. Manter rastreabilidade |
| **Duplicatas em Big Data** | • Locality-Sensitive Hashing (LSH). Min-Hash. Blocking techniques. Sampling e validação | • MapReduce para deduplicação. Processamento distribuído. Sharding por chaves. Deduplicação em stream | • Escalabilidade. Performance. Precisão vs tempo. Distribuição de dados | • Apache Spark. Hadoop. LSH libraries. Distributed deduplication | • Throughput. Escalabilidade. Uso de recursos. Qualidade vs velocidade | • Implementar blocking. Usar hashing eficiente. Paralelizar processamento. Balancear precisão e custo |
| **Duplicatas em Dados Textuais** | • Similaridade de documentos. Análise de n-gramas. Embedding similarity. Topic modeling | • Agrupamento por similaridade. Representação canônica. Citação de fontes. Detecção de variações | • Similaridade semântica. Plágio vs citação. Variações linguísticas. Contexto e significado | • NLTK. spaCy. Gensim. Plagiarism detection tools | • Similaridade textual. Precision/Recall. Cobertura temática. Classificação correta | • Usar múltiplas métricas. Considerar contexto. Implementar thresholds adaptativos. Validar manualmente amostra |
| **Duplicatas em Machine Learning** | • Detecção de contaminação entre conjuntos. Análise de representatividade. Feature fingerprinting. Cross-validation especial | • Particionamento estratificado. Remoção balanceada. Verificação de vazamento. Amostragem representativa | • Overfitting. Data leakage. Enviesamento. Generalização | • scikit-learn. DVC. MLflow. Data drift detection | • Desempenho em validação. Generalização. Robustez do modelo. Estabilidade temporal | • Separar dados antes da limpeza. Verificar contaminação. Documentar processo. Implementar validação cruzada apropriada |